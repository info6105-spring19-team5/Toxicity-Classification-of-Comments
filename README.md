# Toxicity-Classification-of-Comments
INFO 6105 - Final Project

The objective of this project is to build a ML model to detect the presence level of toxicity in the comments posted online. 
•	The six toxicity labels defined for classification are: Obscene, threat, identity attack, sexual_explicit, insult and severe_toxicity.
•	The dataset was curated from Jigsaw’s toxic text classification dataset. Logistic Regression model was implemented.
•	Detection of bias in comment classification was done using  Logistic Regression and CNN.
#### Claat link: https://codelabs-preview.appspot.com/?file_id=1rVfyXG2iSHcEb1IMWnsnDfSg4VnUvcKH6FgYSHEI1VE#1


